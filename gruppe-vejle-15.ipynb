{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport datetime\nimport seaborn as sns\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom numpy import genfromtxt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading datasets"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"calendar_data = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\nsell_price_data = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\nsales_train_validation = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\nsample_submission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Sales Train Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_validation.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_validation.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sepererer Data om hvor meget der er solgt en dag i en liste, væk fra de andre kolonner i sales_train_validation, ved at sortere på string segmentet 'd_'"},{"metadata":{"trusted":true},"cell_type":"code","source":"date_col = [col for col in sales_train_validation if col.startswith('d_')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(date_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_validation.state_id.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"finder total mængde af varer solgt pr vare"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_validation['total_sales'] = sales_train_validation[date_col].sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_validation['total_sales'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Totalt salg pr stat, i antal af varer."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_validation.groupby('state_id').agg({\"total_sales\":\"sum\"}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sales_train_validation.state_id.value_counts())\n#Tilføj ny kolonne til dataset med total salg (summen af alle dato kolonner)\nsales_train_validation['total_sales'] = sales_train_validation[date_col].sum(axis=1)\n#Calculating the sales ratio\ntotal_salg_per_stat = sales_train_validation.groupby('state_id').agg({\"total_sales\":\"sum\"})/sales_train_validation.total_sales.sum() * 100\ntotal_salg_per_stat = total_salg_per_stat.reset_index()\n#Plotting the sales ratio\nfig1, ax1 = plt.subplots()\n#Opret et nyt pie chart vha. matplotlib\nax1.pie(total_salg_per_stat['total_sales'],labels= total_salg_per_stat['state_id'] , autopct='%1.1f%%',\n        shadow=True, startangle=90)# Equal aspect ratio ensures that pie is drawn as a circle\nplt.tight_layout()\nplt.title(\"Total salg per stat\",fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Totalt salg af varer pr butik"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sales_train_validation.groupby('store_id').agg({\"total_sales\":\"sum\"}).reset_index())\n\n#Finder den totale salgsrate fordelt på de enkelte butikker\ntotal_salg_per_butik=sales_train_validation.groupby('store_id').agg({\"total_sales\":\"sum\"})/sales_train_validation.total_sales.sum() * 100\n#Lav et piechart som viser fordelingen i procent\ntotal_salg_per_butik = total_salg_per_butik.reset_index()\nfig1, ax1 = plt.subplots()\nax1.pie(total_salg_per_butik['total_sales'],labels= total_salg_per_butik['store_id'] , autopct='%1.1f%%',\n        shadow=True, startangle=90)# Equal aspect ratio ensures that pie is drawn as a circle\nax1.axis('equal')  \nplt.tight_layout()\nplt.title(\"Totale solgte varer i procent per butik\",fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Totalt salg fordelt i kategorier"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sales_train_validation.groupby('cat_id').agg({\"total_sales\":\"sum\"}).reset_index())\n\n#Finder den totale salgsrate fordelt på de enkelte kategorier\ntotal_salg_per_kategori = sales_train_validation.groupby('cat_id').agg({\"total_sales\":\"sum\"})/sales_train_validation.total_sales.sum() * 100\ntotal_salg_per_kategori = total_salg_per_kategori.reset_index()\n#Lav et piechart som viser fordelingen i procent\nfig1, ax1 = plt.subplots()\nax1.pie(total_salg_per_kategori['total_sales'],labels= total_salg_per_kategori['cat_id'] , autopct='%1.1f%%',\n        shadow=True, startangle=90)# Equal aspect ratio ensures that pie is drawn as a circle\nax1.axis('equal')  \nplt.tight_layout()\nplt.title(\"Totale solgte varer i procent per kategori\",fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_validation.groupby(['state_id','cat_id']).agg({\"total_sales\":\"sum\"}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Salg af varer efter dept_id"},{"metadata":{"trusted":true},"cell_type":"code","source":"dept_sales = sales_train_validation.groupby('dept_id').agg({\"total_sales\":\"sum\"}).reset_index()\nprint(dept_sales)\n#Finder den totale salgsrate fordelt på de enkelte afdelinger\ntotal_salg_per_afdeling = sales_train_validation.groupby('dept_id').agg({\"total_sales\":\"sum\"})/sales_train_validation.total_sales.sum() * 100\n#Lav et piechart som viser fordelingen i procent\ntotal_salg_per_afdeling = total_salg_per_afdeling.reset_index()\nfig1, ax1 = plt.subplots()\nax1.pie(total_salg_per_afdeling['total_sales'],labels= total_salg_per_afdeling['dept_id'] , autopct='%1.1f%%',\n        shadow=True, startangle=90)# Equal aspect ratio ensures that pie is drawn as a circle\nax1.axis('equal')  \nplt.tight_layout()\nplt.title(\"Totale solgte varer i procent per afdeling\",fontweight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sell Price Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_price_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_price_data.groupby(['store_id', 'item_id']).agg({\"sell_price\": [\"max\", \"min\"]}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Undersøgelse af prisændringer for de enkelte items over tid, med min og max værdier\n#Aggregerer min og max prisværdier for de enkelte items i hver butik\nitem_store_prices = sell_price_data.groupby([\"item_id\",\"store_id\"]).agg({\"sell_price\":[\"max\",\"min\"]})\n#print(item_store_prices.head())\n#Ændre navn på kolonner for min og max værdier\nitem_store_prices.columns = [f'{i}_{j}' if j != '' else f'{i}' for i,j in item_store_prices.columns]\n#Tilføjer en ny 'price change' kolonne med prisændring for hver item per butik\nitem_store_prices[\"price_change\"] = item_store_prices[\"sell_price_max\"] - item_store_prices[\"sell_price_min\"]\n#print(item_store_prices.head())\n#Laver en ny data frame med værdierne sorteret efter prisændring\nitem_store_prices_sorted = item_store_prices.sort_values([\"price_change\",\"item_id\"],ascending=False).reset_index()\n#Tilføjer en ny kolonne 'category' med navnet for kategorien per item\nitem_store_prices_sorted[\"category\"] = item_store_prices_sorted[\"item_id\"].str.split(\"_\",expand = True)[0]\n#print(item_store_prices_sorted.head())\n#Boxplot af prisændringer ved brug af seaborn\nsns.boxplot(x=\"price_change\", y=\"category\", data=item_store_prices_sorted)\ntitle = plt.title(\"Boxplot over prisændringer for alle items opdelt efter kategori\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calendar Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pivot er en omdannelse af vores dataframe snap_days så vores år bliver til kolonner, og index'et bliver til måneden. "},{"metadata":{},"cell_type":"markdown","source":"## Snap Days"},{"metadata":{"trusted":true},"cell_type":"code","source":"snap_days = calendar_data.groupby(['year','month'])['snap_CA','snap_TX','snap_WI'].sum().reset_index()\nsnap_days.pivot(index=\"month\",columns = \"year\",values = [\"snap_CA\",\"snap_TX\",\"snap_WI\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_date = datetime.datetime(2011,1,29)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_sum = pd.DataFrame(sales_train_validation[date_col].sum(axis =0),columns = [\"sales\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tilføjer dato kolonne vha. for in range loop\nsales_sum['date'] = [start_date + datetime.timedelta(days=x) for x in range(1913)]\nsales_sum.set_index('date', drop=True, inplace=True)\nprint(sales_sum)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Yearly overall sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_sum.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Det er tydeligt at der er seasonality i datasettet, samt at salget dropper til 0 omkring nytår og juleaften, måske fordi Wallmart har lukket de dage."},{"metadata":{"trusted":true},"cell_type":"code","source":"clndr = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\nclndr['date'] = pd.to_datetime(clndr.date)\nclndr['days'] = clndr['date'].dt.day\n\ndf = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\n\ndata = pd.DataFrame(df.groupby(by= ['cat_id','dept_id','item_id','store_id']).sum())\n\n# Daily Sales data for each category\n\nfood = pd.DataFrame(data.xs('FOODS').sum(axis = 0))\nhobbies = pd.DataFrame(data.xs('HOBBIES').sum(axis = 0))\nhouse = pd.DataFrame(data.xs('HOUSEHOLD').sum(axis = 0))\n\nclndr = pd.merge(clndr,food,how = 'left',left_on=clndr['d'],right_on = food.index)\ndel clndr['key_0']\nclndr.rename(columns = {0:'food'},inplace = True)\nclndr = pd.merge(clndr,hobbies,how = 'left',left_on=clndr['d'],right_on = hobbies.index)\ndel clndr['key_0']\nclndr.rename(columns = {0:'hobby'},inplace = True)\nclndr = pd.merge(clndr,house,how = 'left',left_on=clndr['d'],right_on = house.index)\ndel clndr['key_0']\nclndr.rename(columns = {0:'house'},inplace = True)\n\ncln = clndr[0:1913]\n\nl1 = ['FOODS','HOBBIES','HOUSEHOLD']\nl2 = list(df['store_id'].unique())\nfor cat in l1:\n    for store in l2:\n        tmp = pd.DataFrame(data.xs(cat).xs(store,level = 2 ).sum(axis = 0))\n        tmp.reset_index(inplace = True)\n        tmp.rename(columns = {0:(cat+'_'+store).lower()},inplace = True)\n        cln = pd.concat([cln,tmp[(cat+'_'+store).lower()]],axis = 1)\n\ngrps = cln.groupby(by=['year','month'])\ndef plot_trend(factor,subplots):\n    if subplots == True:\n        f, a = plt.subplots(3,2,figsize = (14,10))\n        if type(factor) == list:\n            for i,fact in enumerate(factor):\n                check = grps.agg(fact=(fact,'sum'))\n                check.rename(columns = {'fact':factor[i]},inplace=True)\n                check.xs(2011).plot(ax=a[0,0])\n                check.xs(2012).plot(ax=a[0,1])\n                check.xs(2013).plot(ax=a[1,0])\n                check.xs(2014).plot(ax=a[1,1])\n                check.xs(2015).plot(ax=a[2,0])\n                check.xs(2016).plot(ax=a[2,1])\n                \n        else:\n            check = grps.agg({factor:'sum'})\n            check.xs(2011).plot(ax=a[0,0])\n            check.xs(2012).plot(ax=a[0,1])\n            check.xs(2013).plot(ax=a[1,0])\n            check.xs(2014).plot(ax=a[1,1])\n            check.xs(2015).plot(ax=a[2,0])\n            check.xs(2016).plot(ax=a[2,1])\n        a[0,0].title.set_text('2011')\n        a[0,1].title.set_text('2012')\n        a[1,0].title.set_text('2013')\n        a[1,1].title.set_text('2014')\n        a[2,0].title.set_text('2015')\n        a[2,1].title.set_text('2016')\n        f.tight_layout()\n        f.suptitle('Monthly Sales Trends')\n    else:\n        fig,ax = plt.subplots(figsize = (20,5))\n        for fact in factor:\n            cln.set_index('date')[fact].rolling(30).mean().plot(label = fact)\n            plt.legend()\n            fig.suptitle('30 Day Moving Average')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_trend(['food','hobby','house'],subplots = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_trend(['food'],subplots = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_trend(['hobby'],subplots = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Der kan ses en stiging i salget af hobbyartikler omkring starten af 2013, da det var året hvor den amerikanske økonomi begyndte at komme sig efter finanskrisen i 2009."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_trend(['house'],subplots = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyse af time series"},{"metadata":{},"cell_type":"markdown","source":"Fordi vi kan se det er et seasonal timeseries, en timeseries består af 3 systematiske komponenter, Trend, level, Seasonality og en ikke-systematisk komponent residuals, benytter vi seasonal_decompose for at undersøge disse.\n\nSystematiske komponenter er dele af timeseries' der har konsistens eller optræder gentagne gange og kan beskrives og modelleres.\n\nIkke-systematiske komponenter er dele af timeseries der ikke kan modeleres direkte.\n\n* Y[t] = T[t] + S[t] + e[t]\n\nTrend er den voksende eller faldende værdi i datasættet.\n\nLevel er den gennemsnittelige værdi i datasættet.\n\nSeasonality er gentagende short-term cycluser i datasættet.\n\nResiduals er støjen der er i datasættet\n\nVi benytter additiv fordi det ligner at datasættet er lineært og ikke expotientielt.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"result = seasonal_decompose(sales_sum, model='additive')\nresult.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar_df = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv', parse_dates=['date'], usecols=['date','d'])\ncalendar_stv = calendar_df[:1913] \n\nsales_train_df = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv', index_col='id')\n\nstore_dept = sales_train_df.groupby(by= ['cat_id'], axis=0).mean()\nstore_dept.columns = calendar_stv['date']\nstore_trans = store_dept.transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weekends = ['01-03-2015','01-04-2015','01-10-2015','01-11-2015','01-17-2015', '01-18-2015','01-24-2015', '01-25-2015', '01-31-2015', \n            '02-01-2015', '02-07-2015', '02-08-2015', '02-14-2015', '02-15-2015', '02-21-2015', '02-22-2015', '02-28-2015', \n            '03-01-2015', '03-07-2015', '03-08-2015', '03-14-2015', '03-15-2015', '03-21-2015', '03-22-2015', '03-28-2015',  '03-29-2015']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = [25, 5]\nax = store_trans['01-01-2015':'04-02-2015'].plot(title=\"Gns. salg 3 måneder jan-mar 2015\")\nax.set_ylabel('# enheder')\nax.vlines(weekends, 0, 2.5, colors=['y','c'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vi kan se at der er en forøgning i salget omkring weekender, hvorfor man kan argumentere for at der er en seasonality på 7 dage, hvilket vi kan anvende i vores ARIMA model."},{"metadata":{},"cell_type":"markdown","source":"Vi tænker derfor at vi vil starte med at forsøge at predicte på dataen med en ARIMA model"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_validation.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = sales_train_validation[date_col[-100:-30]]\nval_dataset = sales_train_validation[date_col[-30:]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vi vælger at lave en sarimax model med en seasonality på 7, da vi ud fra vores seasonal_decomposition fra tidl. kunne se at der er seasonality på ugentligt basis,  hvor at 7 er en uge lang.\n"},{"metadata":{},"cell_type":"markdown","source":"# ARIMA Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\nsummary = []\n##An ARMA model is an ARIMA model where the d parameter in the order is 0\nfor row in (train_dataset[train_dataset.columns[-100:]].values[:3]):\n    print(row)\n    fit = SARIMAX(row, seasonal_order=(1, 1, 1, 7)).fit()\n    predictions.append(fit.forecast(30))\n    summary.append(fit.summary())\n    fit.plot_diagnostics()\npredictions = np.array(predictions).reshape((-1, 30))\nerror_arima = np.linalg.norm(predictions[:3] - val_dataset.values[:3])/len(predictions[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vi kan se at på 2 ud af de 3 fittede modeler er det ikke særligt godt.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(predictions)\nprint(summary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sammenligning af predictions og af de faktiske values"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_1 = predictions[0]\npred_2 = predictions[1]\npred_3 = predictions[2]\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"),\n               name=\"Val\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_1, mode='lines', marker=dict(color=\"seagreen\"),\n               name=\"Pred\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_2, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_3, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"ARIMA\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Egentlig forecast til aflevering:"},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_dataset = sales_train_validation[sales_train_validation.columns[-500:]].values[:5]\n\nprint(predict_dataset)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looper igennem alle produkter og foretager predictions med en sarimax 0117 model for de næste 28 dage."},{"metadata":{"trusted":true},"cell_type":"code","source":"#results = []\n#for row in (sales_train_validation[sales_train_validation.columns[-100:]].values):\n#    fit = SARIMAX(row, seasonal_order=(1, 1, 1, 7)).fit()\n#    results.append(fit.forecast(28))\n#print(results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Det tog 2 timer at køre ovenstående vi har derfor gemt resultatet i en CSV til senere brug."},{"metadata":{"trusted":true},"cell_type":"code","source":"results = genfromtxt(\"sarimax-predictions.csv\", delimiter=',')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Model"},{"metadata":{},"cell_type":"markdown","source":"Her 'cluster' vi data fra de 3 kategorier og de 3 stater i hver sit dataset, som vi så samler i 'clus'. "},{"metadata":{"trusted":true},"cell_type":"code","source":"column_index = [1,2,3,4,5]\nfor i in range(6 , len(sales_train_validation.columns)):\n    column_index.append(i)\n\nclus_hobbies = sales_train_validation.iloc[:,column_index].query(\"cat_id == 'HOBBIES'\")\nclus_household = sales_train_validation.iloc[:,column_index].query(\"cat_id == 'HOUSEHOLD'\")\nclus_foods = sales_train_validation.iloc[:,column_index].query(\"cat_id == 'FOODS'\")\nclus_ca = sales_train_validation.iloc[:,column_index].query(\"state_id == 'CA'\")\nclus_tx = sales_train_validation.iloc[:,column_index].query(\"state_id == 'TX'\")\nclus_wi = sales_train_validation.iloc[:,column_index].query(\"state_id == 'WI'\")\nclus = sales_train_validation.iloc[:,column_index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tilføjer nye kolonner til calendar_data, med datoer opdelt. "},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar_data[\"event_type_1_snap\"] = pd.notna(calendar_data[\"event_type_1\"]) \ncalendar_data[\"event_type_2_snap\"] = pd.notna(calendar_data[\"event_type_2\"]) \ncalendar_data[\"date\"] =  pd.to_datetime(calendar_data[\"date\"])\ncalendar_data[\"d_month\"] = calendar_data[\"date\"].dt.day\ncalendar_data[\"year\"] = pd.to_numeric(calendar_data[\"year\"])\ncalendar_data[\"wday\"] = pd.to_numeric(calendar_data[\"wday\"])\nprint(calendar_data.shape)\ncalendar_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Samler kolonner efter månedens dage i et 'columnsets' data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bucket columns by calander days of month\ncolumnsets = []\nfor i in range(1,32):      \n    d = calendar_data[:1913].query(\"d_month == \"+ str(i))[\"d\"]\n    columnsets.append([d.values])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Metode til at encode de enkelte features til den rette datatype."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label encoding for catagorical data\ndef label_encoding(data_preap,cat_features):\n    categorical_names = {}\n    data = []\n    encoders = []\n    \n    data = data_preap[:]\n    for feature in cat_features:\n        le = LabelEncoder()\n        le.fit(data.iloc[:,feature])\n        data.iloc[:, feature] = le.transform(data.iloc[:, feature])\n        categorical_names[feature] = le.classes_\n        encoders.append(le)\n    X_data = data.astype(float)\n    return X_data, encoders","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Metode til træning af RandomForestRegressor model med 350 træer. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training random forest model\ndef train_model(X_train, X_test, Y_train, Y_test):\n    # Random forest regressor model with Training dataset\n    start_time = datetime.today()\n    regressor = RandomForestRegressor(n_estimators = 350, random_state = 50)\n    regressor.fit(X_train,Y_train)\n\n    print(\"Time taken to Train Model: \" + str(datetime.today() - start_time))\n\n    # Running Regession model score check\n    Y_score = regressor.score(X_test,Y_test)\n    return regressor,Y_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Metode til at lave predictions på test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict function from model\ndef model_predict(regressor,X_data):\n    # Predicting model model result\n    Y_pred = regressor.predict(X_data)\n    return Y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validating model with last year data & generating rmse value for the model predection\ndef validate_model(regressor,X_validation, Y_validation):\n   \n    Y_validation_pred = model_predict(regressor, X_validation)\n    mse = mean_squared_error(Y_validation, Y_validation_pred)\n    rmse = np.sqrt(mse)\n    return rmse, Y_validation_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic function for geting data from pandas based on range\ndef get_data_range(Inital_Range,start_index,end_index):\n    result = []\n    [result.append(a) for a in Inital_Range]\n    for i in range(max(Inital_Range) +1 + start_index, end_index):\n        result.append(i)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# main function to run predictions\ndef run_predictions(orig_data):\n    process_data = orig_data[:]\n    results = pd.DataFrame()\n    for s in range(1,29):\n        categorical_features = [0,1]\n        data = []\n        data_range = []\n        for i in range(0,s):\n            [data_range.append(a) for a in columnsets[i]]\n        data_list = [process_data[a] for a in data_range]\n        data  = pd.concat(data_list,axis = 1)\n\n\n        data.insert(loc=0, column='item_id', value=process_data[\"item_id\"])\n        data.insert(loc=1, column='store_id', value=process_data[\"store_id\"])\n        X_data_preap = data[:]\n\n        d = get_data_range(categorical_features,0,len(X_data_preap.columns)-1)   \n        X,label_encoders = label_encoding(X_data_preap.iloc[:,d],categorical_features)\n        Y = X.iloc[:,-1]\n\n        d_validation = get_data_range(categorical_features,1,len(X_data_preap.columns))   \n        X_validation,label_encoders_validation = label_encoding(X_data_preap.iloc[:,d_validation],categorical_features)\n        Y_validation = X_validation.iloc[:,-1]\n\n        print(\"Running Model for Day \" + str(s))\n        # Sampling data for train & split\n        X_train, X_test, Y_train, Y_test = train_test_split(X.iloc[:,0:len(X.columns)-1],Y,test_size = 0.2, random_state = 0)\n        model, score = train_model(X_train, X_test, Y_train, Y_test)\n        print(\"Model Score: \" + str(score))\n        \n       # Uncomment for inital model\n        rmse,validation_predictions = validate_model(model,X_validation.iloc[:,0:len(X_validation.columns)-1], Y_validation)\n        print(\"RMSE Result: \" + str(rmse))\n        \n        if (len(results.columns) == 0):\n            for feature in categorical_features:\n                results[feature] = label_encoders_validation[feature].inverse_transform(X_validation.iloc[:,feature].astype(int))\n\n        results[\"d_\" + str(s)] = validation_predictions.astype(int)\n        print(results)\n        results.to_csv('pd_predictions_' + str(s) +'.csv')\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd_predictions = run_predictions(clus)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}